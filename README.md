# Bi-LSTM Dependency Parser

Deep Learning Bi-LSTM Implementation for Dependency Parsing, based on Kipperwasser & Goldberg 16'

First navigate to current code folder. Then,

To generate the labeled files that we submitted run:

`python generate_comp_tagged.py`

This will load the pretrained models stored under `models/model<id>.pth`.

The labeled files generated by both models will be stored in same folder as the code,
with the name `comp_m<model_id>_<id>.labeled`

To train model `<model_id>` run:

  - `python main.py --model_id=<model_id>`

To recreate the model we submitted run:
  - `python main.py --model_id=1`
  or `python main.py --model_id=2`


To skip the training process and load pretrained model `<path_to_model>` run:

  - `python main.py --skip_train --model_path=<path_to_model>`

For example:

  - `python main.py --model_path=models/model2.pth`


It is also possible to run cross validation with the `--do_cv` flags.

For the full options:

```bash
usage: main.py [-h] [--skip_train] [--model_path MODEL_PATH] [--msg MSG]
               [--n_epochs_stop N_EPOCHS_STOP] [--model_id MODEL_ID] [--comp]
               [--debug] [--combined_train_data] [--do_cv]

optional arguments:
  -h, --help            show this help message and exit
  --skip_train          if skip train
  --model_path MODEL_PATH
                        if skip train - path model to load
  --msg MSG             msg to write in log file
  --n_epochs_stop N_EPOCHS_STOP
                        early stopping in training
  --model_id MODEL_ID   should be 1 or 2
  --comp
  --debug
  --combined_train_data
  --do_cv
```

